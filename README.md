<h1 align="center">Apache Spark</h1> <br>
<h2> ðŸš€ Table of Contents ðŸš€ </h2>

- [About](#about)
- [Benefit](#benefits)
- [Info](#info)
- [Use-cases](#use-case)
- [Components](#components)
- [Setup](#setup)

## About
Apache Spark is a data processing framework that can quickly perform processing tasks on very large data sets, and can also distribute data processing tasks across multiple computers, either on its own or in tandem with other distributed computing tools.

Apache Spark is written in Scala and because of its scalability on JVM - Scala programming is most prominently used programming language, by big data developers for working on Spark projects. Developers state that using Scala helps dig deep into Sparkâ€™s source code so that they can easily access and implement the newest features of Spark. Scalaâ€™s interoperability with Java is its biggest attraction as java developers can easily get on the learning path by grasping the object oriented concepts quickly.

## Benefit
You can learn in this project:
- Basic Scala (OOP & FP)
- Spark SQL (RDD & DataFrame &DataSet)
- Spark & Cluster
- Spark ML
- Spark Streaming
- Structured-Streaming
- Spark with Notebook

## Info
- https://www.scala-lang.org
- https://spark.apache.org/docs/latest/
- https://intellij-support.jetbrains.com/hc/en-us

## Core Components

- RDD
- DataFrame
- DataSet (RDD + DataFrame)

### Spark provides all data Types
- unstructured data (schema-never)
- semi-structured data (schema-later)
- structured data (schema-first)


## Setup
### [Scala](https://github.com/yuyatinnefeld/spark/tree/master/kafka)
### [Python](https://github.com/yuyatinnefeld/kafka/tree/master/java)
### [Java](https://github.com/yuyatinnefeld/kafka/tree/master/scala)


## Core Concept

### Working with Datasets
Actions
- show()
- collect()
- save()
- count()

Transformation (L)
- select
- distinct
- groupBy
- sum
- agg
- orderBy
- filter
- limit

